---
title: "Learning k-means"
header:
excerpt: "One of the simplest algorithms which uses unsupervised machine learning to solve many clasification problems, i.e. Search engine providing near search teams that are related and medical applications such as cancer detection."
categories:
  - Data-Science
tags:
  - Tutorial
last_modified_at: 2016-01-22T15:11:19-04:00
---
Easy to use and one of the simplest algorithms which uses unsupervised learning that solves many clustering problems e.g. Search engine providing near search teams that are related, Academics when looking at average student performance and medical applications such as cancer detection.

![](/assets/images/k_means/kmeans.png)

## k-means clustering on iris dataset

{% highlight ruby %}
set.seed(8953)
iris2 <- iris
iris2$Species <- NULL
(kmeans.result <- kmeans(iris2, 3))
{% endhighlight %}

#### Output
{% highlight ruby %}
K-means clustering with 3 clusters of sizes 38, 50, 62

Cluster means:
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     6.850000    3.073684     5.742105    2.071053
2     5.006000    3.428000     1.462000    0.246000
3     5.901613    2.748387     4.393548    1.433871

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 [71] 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 1
[106] 1 3 1 1 1 1 1 1 3 3 1 1 1 1 3 1 3 1 3 1 1 3 3 1 1 1 1 1 3 1 1 1 1 3 1
[141] 1 1 3 1 1 1 3 1 1 3
{% endhighlight %}

Within cluster sum of squares by cluster:
[1] 23.87947 15.15100 39.82097
(between_SS / total_SS =  88.4 %)

Available components:
{% highlight ruby %}
[1] "cluster"      "centers"      "totss"        "withinss"
[5] "tot.withinss" "betweenss"    "size"         "iter"
[9] "ifault"
{% endhighlight %}

### Results of k-Means Clustering

Checking clustering result against class labels (Species)

{% highlight ruby %}
table(iris$Species, kmeans.result$cluster)
{% endhighlight %}

#### Output
{% highlight ruby %}
            1  2  3
setosa      0 50  0
versicolor  2  0 48
virginica  36  0 14
{% endhighlight %}

* Class "Setosa" can be easily seperated from the other clusters
* Classes "versicolor" and "virginca" are to a small degree overlapped with each other

### Plot Results
{% highlight ruby %}
plot(iris2[c("Sepal.Length", "Sepal.Width")], col=kmeans.result$cluster)
points(kmeans.result$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=2)
{% endhighlight %}

![](/assets/images/k_means/unnamed-chunk-3-1.png)

### Clustering with Pamk()

K-medoids clustering on iris dataset

{% highlight ruby %}
library(fpc)
pamk.result <- pamk(iris2)

# Number of Clusters
pamk.result$nc

## [1] 2

# Check clustering against actual species
table(pamk.result$pamobject$clustering, iris$Species)
{% endhighlight %}

#### Output
{% highlight ruby %}
  setosa versicolor virginica
1     50          1         0
2      0         49        50
{% endhighlight %}

* K-medoids looks at objects closest to the cent er of the cluster
* Two Clusters: "setosa" and a mixture of "versicolor" and "virginica".

{% highlight ruby %}
layout(matrix(c(1,2),1,2)) # 2 graphs per page
plot(pamk.result$pamobject)
{% endhighlight %}

![](/assets/images/k_means/unnamed-chunk-5-1.png)

{% highlight ruby %}
layout(matrix(1)) # change back to one graph per page
{% endhighlight %}

* The left chart is a 2 dimensional clustering plot of the two clusters and the lines show the distance between clusters
* The right chart shows their silhouettes. A large 'si' (almost 1) suggests that the corresponding observation are very well clustered, a small si (around 0) means that the observation lies between two clusters, and observations with a negative 'si' are probably placed in the wrong cluster
* Since the average 'si' are respectively 0.81 and 0.62 in the above silhouette, the identified two clusters are well clustered.

### Clustering with pam()

{% highlight ruby %}
library(cluster)
# group into 3 clusters
pam.result <- pam(iris2,3)
table(pam.result$clustering, iris$Species)
{% endhighlight %}

#### Output
{% highlight ruby %}
  setosa versicolor virginica
1     50          0         0
2      0         48        14
3      0          2        36
{% endhighlight %}

Three clusters Identified:
* Cluster 1 is species 'setosa' and is well separated from the other two.
* Cluster 2 is mainly composed of 'versicolor', plus some cases from 'virginica'.
* The majority of cluster 3 are 'virginica'. with two cases from 'versicolor'.

{% highlight ruby %}
layout(matrix(c(1,2),1,2)) # 2 graphs per page
plot(pam.result)
{% endhighlight %}

![](/assets/images/k_means/unnamed-chunk-7-1.png)

{% highlight ruby %}
layout(matrix(1)) # change back to one graph per page
{% endhighlight %}

### Results of Clustering

* In this example, the result of pam() seems better, because it identifies three clusters, corresponding to three species
* Note that we cheated by setting k - 3 when using pam(), which is already known to us as the number of species.

### Hierarchical Clustering

{% highlight ruby %}
set.seed(2835)
# draw a sample of 40 records from the iris data, so that the clustering 
# plot will not be over crowded
idx <- sample(1:dim(iris)[1],40)
iris_sample <- iris[idx,]
# remove class label
iris_sample$Species <- NULL
# Hierarchical Clustering
hc<- hclust(dist(iris_sample), method = "ave")
# plot clusters
plot(hc, hang =-1, labels = iris$Species[idx])
# cut tree into three clusters
rect.hclust(hc, k=3)
{% endhighlight %}

![](/assets/images/k_means/unnamed-chunk-8-1.png)

{% highlight ruby %}
# get cluster IDs
groups <- cutree(hc, k=3)
{% endhighlight %}

### Density-based Clustering

* Groups objects into one cluster if they are connected to one another by densely populated area
* The DBSCAN algorithm from package fpc provides a density-based clustering for numeric data.
* Two key parameters in DBSCAN are eps reachability distance which defines the size of neighbourhood and minimum number of points.
* Insensitive to noise. The k-means algorithm tends to find clusters with sphere shape and with simular sizes.

### Density-based Clustering

{% highlight ruby %}
library(fpc)
iris2 <- iris[-5] # remove class tags
ds <- dbscan(iris2, eps = 0.42, MinPts = 5)
# compare clusters wih original class labels
table(ds$cluster, iris$Species)
{% endhighlight %}

#### Output
{% highlight ruby %}
  setosa versicolor virginica
0      2         10        17
1     48          0         0
2      0         37         0
3      0          3        33
{% endhighlight %}

* 1 to 3: identified clusters, 0: noises or outliers, i.e. objects that are not assigned to any clusters

{% highlight ruby %}
plot(ds, iris2)
plot(ds, iris2[c(1,4)])
plotcluster(iris2, ds$cluster)
{% endhighlight %}

### Prediction with Clustering Model

* Label new data, based on their similarity with the clusters
* Draw a sample of 10 objects from iris and add small noise to them to make a new data set for labelling
* Random noises are generated with a uniform distribution using function runif().

{% highlight ruby %}
# Create a new dataset for labelling
set.seed(435)
idx <- sample (1:nrow(iris), 10)
# remove class labels
new.data <- iris[idx,-5]
# add random noise
new.data <- new.data + matrix(runif(10*4, min=0, max=0.2), nrow=10, ncol=4)

#label new data
pred <- predict(ds, iris2, new.data)
{% endhighlight %}

### Results of Prediction

{% highlight ruby %}
table(pred, iris$Species[idx]) # check cluster labels
{% endhighlight %}

#### Output
{% highlight ruby %}
pred setosa versicolor virginica
   0      0          0         1
   1      3          0         0
   2      0          3         0
   3      0          1         2
{% endhighlight %}

* Eight(=3+3+2) our of 10 objects are assigned with correct class labels.default(

{% highlight ruby %}
plot(iris2[c(1,4)], col=1+ds$cluster)
points(new.data[c(1,4)],pch="+", col=1+pred,cex=3)
{% endhighlight %}

![](/assets/images/k_means/unnamed-chunk-12-1.png)
